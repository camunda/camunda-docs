---
id: agentic-ai-ad-hoc-tools-schema-resolver
sidebar_label: Ad-Hoc Tools Schema Resolver
title: Ad-Hoc Tools Schema Resolver connector
description: Resolve an input schema used in combination with LLMs for activities defined within an ad-hoc sub-process.
---

The **Ad-Hoc Tools Schema connector** is an outbound connector that implements the tool resolution part of
the [**AI Agent connector**](./agentic-ai-aiagent.md). While its function is also embedded in the AI Agent connector,
the tools-schema connector can be used independently in combination with other AI connectors.

## Prerequisites

To use the **Ad-Hoc Tools Schema connector**, your process must contain an ad-hoc sub-process whose ID you
can reference. Within the ad-hoc sub-process, activities matching the tool resolution logic will be considered as
_tools_.

To define the input parameters of these tools, the
[`fromAi`](../../modeler/feel/builtin-functions/feel-built-in-functions-miscellaneous.md#fromaivalue) function can be
used
in input/output mappings to define data which needs to be generated by an AI integration (such as the AI Agent
connector) when calling the tool.

## Create an Ad-Hoc Tools Schema connector task

1. Create a service task.
2. [Apply](../use-connectors/outbound.md) the **Ad-Hoc Tools Schema** element template.
3. Configure the **Ad-hoc sub-process ID** to reference the element ID of the ad-hoc sub-process.
4. Configure [a result variable or a result expression](../use-connectors/#variableresponse-mapping) to map the
   connector results to process variables.

## Tool Resolution Logic

When resolving the available "tools" within an ad-hoc sub-process, the connector will take all activities into account
which **have no incoming flows** (root nodes within the ad-hoc sub-process) and **are not boundary events**. In the
following screenshot, the activities marked in red are the ones that will be considered as tools:

![agenticai-tool-resolution.png](../img/agenticai-tool-resolution.png)

## Tool Definitions

A tool definition consists of the following properties:

- **name**: The name of the tool. This is the **ID of the activity** in the ad-hoc sub-process.
- **description**: The description of the tool, telling an AI integration the purpose of the tool. If the
  **documentation** of the activity is set, it will be used as the description. Otherwise, the **name** of the activity
  will be used.
- **inputSchema**: The input schema of the tool, describing the input parameters of the tool. This is a JSON Schema
  object. The connector will look for all input/output mappings of the activity and will create
  a [JSON Schema](https://json-schema.org/) based on the
  [`fromAi`](../../modeler/feel/builtin-functions/feel-built-in-functions-miscellaneous.md#fromaivalue) function calls
  found in the mappings. If no `fromAi` function calls are found, an empty JSON Schema object will be returned.

## AI-generated parameters via `fromAi`

Within an activity, you can define parameters which should be AI-generated by tagging them with the
[`fromAi`](../../modeler/feel/builtin-functions/feel-built-in-functions-miscellaneous.md#fromaivalue) FEEL function in
input/output mappings.

The function itself does not implement any logic (it simply returns the first argument it receives), but provides a way
to configure all the necessary metadata (e.g. description, type) to generate an input schema definition. The tools
schema connector will collect all `fromAi` definitions within an activity and combine them into an input schema for
the activity.

:::note
The first argument passed to the `fromAi` function must be a reference type (e.g. not a static string) as this
reference can be used by an AI integration to inject the value.
:::

:::important
It depends on the AI integration you are using in combination with the tool resolving, but for the `AI Agent` connector
it is important that the input is referencing a variable which is used to store all the input parameters for an
individual tool execution. For example `toolCall.myParameter` (`toolCall` containing all variables for the individual
tool call).
:::

You can use the `fromAi` function in:

- Input & Output mappings (e.g Service Task, Script Task, User Task)
- Custom input fields provided by an element template if an element template is applied to the activity as technically
  these are handled as input mappings.

An example of `fromAi` function usage on a [REST outbound connector](../protocol/rest.md):

![agenticai-tool-resolution-fromAi.png](../img/agenticai-tool-resolution-fromAi.png)

### `fromAi` examples

The [`fromAi`](../../modeler/feel/builtin-functions/feel-built-in-functions-miscellaneous.md#fromaivalue) FEEL function
can be called with a varying number of parameters to define simple or complex inputs. The simplest form is to just pass
a value:

```feel
fromAi(toolCall.url)
```

To make a LLM understand the purpose of the input, you can add a description:

```feel
fromAi(toolCall.url, "The URL to download the file from. Should be an RFC 3986 compliant HTTP/HTTPS URL.")
```

To define the type of the input, you can add a type (if no type is given, it will default to "string"):

```feel
fromAi(toolCall.firstNumber, "The first number.", "number")

fromAi(toolCall.shouldCalculate, "Defines if the calculation should be executed", "boolean")
```

For more complex type definitions, the fourth parameter of the function allows you to specify a JSON Schema (must be a
FEEL context). Note that support for the JSON Schema features is depending on the AI integration:

```feel
fromAi(
  toolCall.myComplexObject,
  "A complex object",
  "string",
  { enum: ["first", "second"] }
)
```

## Response Structure

The connector returns a list of tool definitions in the following format. Individual tool definitions are modeled after
the [list tools response](https://modelcontextprotocol.io/specification/2025-03-26/server/tools#listing-tools) defined
in the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) specification, so you should be able to directly
use the definitions with different LLMs or transform them into the reuired format for your AI integration.

```json
{
  "toolDefinitions": [
    {
      "name": "GetDateAndTime",
      "description": "Returns the current date and time including the timezone.",
      "inputSchema": {
        "type": "object",
        "properties": {},
        "required": []
      }
    },
    {
      "name": "Download_A_File",
      "description": "Download a file from the provided URL",
      "inputSchema": {
        "type": "object",
        "properties": {
          "url": {
            "type": "string",
            "description": "The URL to download the file from"
          }
        },
        "required": ["url"]
      }
    },
    {
      "name": "SuperfluxProduct",
      "description": "Calculates the superflux product (a very complicated calculation) given two input numbers",
      "inputSchema": {
        "type": "object",
        "properties": {
          "a": {
            "type": "number",
            "description": "The first number to be superflux calculated."
          },
          "b": {
            "type": "number",
            "description": "The second number to be superflux calculated."
          }
        },
        "required": ["a", "b"]
      }
    }
  ]
}
```

You can either configure a result variable to contain the whole response or use a result expression to map parts of the
response into your process.
